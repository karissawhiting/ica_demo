lda.te=mean(qda.pred$class!=test$chd); lda.te # Test error rate =  0.2592
lda.se=sd(qda.pred$class!=test$chd)/sqrt(162); lda.se # S.E. = 0.035
sum.table=data.frame(Logistic=c(log.te, log.se),
LDA=c(lda.te, lda.se),
QDA=c(qda.te, qda.se))
## LDA
lda.fit=lda(chd~., data=train)
lda.fit
lda.pred=predict(lda.fit, newdata = test[,1:9])
table(lda.pred$class, test$chd)
lda.te=mean(lda.pred$class!=test$chd); lda.te # Test error rate = 0.2531
lda.se=sd(lda.pred$class!=test$chd)/sqrt(162); lda.se # S.E. = 0.034
## QDA
qda.fit=qda(chd~., data=train)
qda.fit
qda.pred=predict(qda.fit, newdata = test[,1:9])
table(qda.pred$class, test$chd)
qda.te=mean(qda.pred$class!=test$chd); qda.te # Test error rate =  0.2592
qda.se=sd(qda.pred$class!=test$chd)/sqrt(162); qda.se # S.E. = 0.035
sum.table=data.frame(Logistic=c(log.te, log.se),
LDA=c(lda.te, lda.se),
QDA=c(qda.te, qda.se))
row.names(sum.table)=c("Training error", "Training s.e.")
sum.table
row.names(sum.table)=c("Training error", "Training error s.e.")
sum.table
knitr::opts_chunk$set(echo = TRUE,
eval = TRUE,
warning = FALSE,
message = FALSE)
#setwd("~/Dropbox/Columbia-Dropbox/Study/Fall 2017/P9210 Stat learning/Homeworks/")
rm( list = ls() )
library(splines)
library(ElemStatLearn)
library(MASS)
library(ggplot2)
sum.table=data.frame(Logistic=c(log.te, log.se),
LDA=c(lda.te, lda.se),
QDA=c(qda.te, qda.se))
lda.fit
data=SAheart
dim(data)
summary(data)
# Split training and test sets
train=data[1:300,]
test=data[301:nrow(data),]
## Logistic regression
log.reg=glm(chd~., data=train, family = binomial())
log.probs=predict(log.reg, newdata=test[,1:9]) # Predicted probs of test data
log.pred=rep(1, 162)
log.pred[log.probs<0.5]=0 # Define Prob < 0.5 -- chd = 0, Prob ≥ 0.5 -- chd = 1
table(log.pred, test$chd)
log.te=mean(log.pred!=test$chd); log.te # Test error rate = 0.2469
log.se=sd(log.pred==test$chd)/sqrt(162); log.se # S.E. = 0.034
## LDA
lda.fit=lda(chd~., data=train)
lda.fit
lda.pred=predict(lda.fit, newdata = test[,1:9])
table(lda.pred$class, test$chd)
lda.te=mean(lda.pred$class!=test$chd); lda.te # Test error rate = 0.2531
lda.se=sd(lda.pred$class!=test$chd)/sqrt(162); lda.se # S.E. = 0.034
## QDA
qda.fit=qda(chd~., data=train)
qda.fit
qda.pred=predict(qda.fit, newdata = test[,1:9])
table(qda.pred$class, test$chd)
qda.te=mean(qda.pred$class!=test$chd); qda.te # Test error rate =  0.2592
qda.se=sd(qda.pred$class!=test$chd)/sqrt(162); qda.se # S.E. = 0.035
sum.table=data.frame(Logistic=c(log.te, log.se),
LDA=c(lda.te, lda.se),
QDA=c(qda.te, qda.se))
row.names(sum.table)=c("Training error", "Training error s.e.")
sum.table
summary(lda.)
summary(lda.fit)
summary(qda.fit)
lda.fit
qda.fit
knitr::opts_chunk$set(echo = TRUE,
eval = TRUE,
warning = FALSE,
message = FALSE)
#setwd("~/Dropbox/Columbia-Dropbox/Study/Fall 2017/P9210 Stat learning/Homeworks/")
rm( list = ls() )
library(splines)
library(ElemStatLearn)
library(MASS)
library(ggplot2)
set.seed(1026)
x=runif(50, 0, 1)
length(x)
## b
train.y=data.frame(matrix(rep(NA, 5000), ncol=100))
for(i in 1:100){
set.seed(2017+i)
train.y[,i]=sin(2*pi*x^3)^3+rnorm(50)
}
colnames(train.y)=1:100
train.y[1:5,1:5]
ols.lm.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(ols.lm.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
ols.lm=lm(y~x, data=temp.df) # OLS fit
ols.lm.fit[,i]=ols.lm$fitted.values # Store fitted values
}
ols.lm.fit[1:5, 1:5]
#dim(ols.lm.fit)
#### OLS cubic
ols.cu.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(ols.cu.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
ols.cu.lm=lm(y~poly(x, degree = 3), data=temp.df) # OLS poly fit
ols.cu.fit[,i]=ols.cu.lm$fitted.values # store fitted values
}
#test=lm(y~poly(x, 3), data=temp.df)
#summary(test)
ols.cu.fit[1:5, 1:5]; dim(ols.cu.fit)
### Cubic spline
bs.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(bs.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
bs.lm=lm(y~bs(x, knots = c(0.33, 0.66)), data=temp.df) # BS fit
bs.fit[,i]=bs.lm$fitted.values # Store fitted values
}
bs.fit[1:5, 1:5]; dim(bs.fit)
### Natural cubic spline
ncs.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(ncs.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
ncs.lm=lm(y~ns(x, df = 3, knots = c(0.1, 0.3, 0.5, 0.7, 0.9)), data=temp.df) # BS fit
ncs.fit[,i]=ncs.lm$fitted.values # Store fitted values
}
ncs.fit[1:5, 1:5]; dim(ncs.fit)
### Smoothing spline
ss.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(ss.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
ss.lm=smooth.spline(temp.df, cv=F) # SS fit
ss.fit[1:length(ss.lm$y),i]=ss.lm$y # Store fitted values
}
ss.fit[1:5, 1:5]; dim(ss.fit)
## d
var.fit=data.frame(matrix(rep(NA, 250), ncol=5)) # DF to store all pointwise variance
colnames(var.fit)=c("fit_1", "fit_2", "fit_3", "fit_4", "fit_5")
all_y=list(ols.lm.fit, ols.cu.fit, bs.fit, ncs.fit, ss.fit)
for(i in 1:length(all_y)){
data=all_y[[i]]
var.fit[,i]=apply(data, 1, var)
}
var.fit[,6]=x; colnames(var.fit)[6]="X" # Add X into the df
var.fit[1:5,]
var.fit2=reshape(var.fit, v.names = "Value", direction = "long",
varying = colnames(var.fit)[1:5], timevar = "Method") # Reshape var.fit to long form
var.fit2[,4]=NULL
var.fit2$Method=factor(var.fit2$Method)
levels(var.fit2$Method)=c("Glocal Linear","Global Cubic Polynomial", "Cubic Spline-2 Knots", "Natural Cubic Splines-5 Knotes","Smooth Splines with GCV")
ggplot(data=var.fit2, aes(x=X, y=Value))+geom_line(aes(col=Method,group=Method))+
geom_point(aes(col=Method,group=Method))+
scale_color_manual("Methods", values = c("orange", "red", "green","blue","purple"))+ylab("Pointwise Variances") + theme_bw()
data=SAheart
dim(data)
summary(data)
# Split training and test sets
train=data[1:300,]
test=data[301:nrow(data),]
## Logistic regression
log.reg=glm(chd~., data=train, family = binomial())
log.probs=predict(log.reg, newdata=test[,1:9]) # Predicted probs of test data
log.pred=rep(1, 162)
log.pred[log.probs<0.5]=0 # Define Prob < 0.5 -- chd = 0, Prob ≥ 0.5 -- chd = 1
table(log.pred, test$chd)
log.te=mean(log.pred!=test$chd); log.te # Test error rate = 0.2469
log.se=sd(log.pred==test$chd)/sqrt(162); log.se # S.E. = 0.034
## LDA
lda.fit=lda(chd~., data=train)
lda.fit
lda.pred=predict(lda.fit, newdata = test[,1:9])
table(lda.pred$class, test$chd)
lda.te=mean(lda.pred$class!=test$chd); lda.te # Test error rate = 0.2531
lda.se=sd(lda.pred$class!=test$chd)/sqrt(162); lda.se # S.E. = 0.034
## QDA
qda.fit=qda(chd~., data=train)
qda.fit
qda.pred=predict(qda.fit, newdata = test[,1:9])
table(qda.pred$class, test$chd)
qda.te=mean(qda.pred$class!=test$chd); qda.te # Test error rate =  0.2592
qda.se=sd(qda.pred$class!=test$chd)/sqrt(162); qda.se # S.E. = 0.035
pnorm(0.2)
2*pnorm(0.2, lower.tail = F)
pnorm(-0.4)
pnorm(-0.1)
knitr::opts_chunk$set(echo = TRUE,
eval = TRUE,
warning = FALSE,
message = FALSE)
#setwd("~/Dropbox/Columbia-Dropbox/Study/Fall 2017/P9210 Stat learning/Homeworks/")
rm( list = ls() )
library(splines)
library(ElemStatLearn)
library(MASS)
library(ggplot2)
set.seed(1026)
x=runif(50, 0, 1)
length(x)
## b
train.y=data.frame(matrix(rep(NA, 5000), ncol=100))
for(i in 1:100){
set.seed(2017+i)
train.y[,i]=sin(2*pi*x^3)^3+rnorm(50)
}
colnames(train.y)=1:100
train.y[1:5,1:5]
ols.lm.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(ols.lm.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
ols.lm=lm(y~x, data=temp.df) # OLS fit
ols.lm.fit[,i]=ols.lm$fitted.values # Store fitted values
}
ols.lm.fit[1:5, 1:5]
#dim(ols.lm.fit)
#### OLS cubic
ols.cu.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(ols.cu.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
ols.cu.lm=lm(y~poly(x, degree = 3), data=temp.df) # OLS poly fit
ols.cu.fit[,i]=ols.cu.lm$fitted.values # store fitted values
}
#test=lm(y~poly(x, 3), data=temp.df)
#summary(test)
ols.cu.fit[1:5, 1:5]; dim(ols.cu.fit)
### Cubic spline
bs.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(bs.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
bs.lm=lm(y~bs(x, knots = c(0.33, 0.66)), data=temp.df) # BS fit
bs.fit[,i]=bs.lm$fitted.values # Store fitted values
}
bs.fit[1:5, 1:5]; dim(bs.fit)
### Natural cubic spline
ncs.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(ncs.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
ncs.lm=lm(y~ns(x, df = 3, knots = c(0.1, 0.3, 0.5, 0.7, 0.9)), data=temp.df) # BS fit
ncs.fit[,i]=ncs.lm$fitted.values # Store fitted values
}
ncs.fit[1:5, 1:5]; dim(ncs.fit)
### Smoothing spline
ss.fit=data.frame(matrix(rep(NA, 5000), ncol=100))
colnames(ss.fit)=1:100
for(i in 1:100){
temp.df=data.frame(x=x,
y=train.y[,i]) # Temp dataframe to store x and y
ss.lm=smooth.spline(temp.df, cv=F) # SS fit
ss.fit[1:length(ss.lm$y),i]=ss.lm$y # Store fitted values
}
ss.fit[1:5, 1:5]; dim(ss.fit)
## d
var.fit=data.frame(matrix(rep(NA, 250), ncol=5)) # DF to store all pointwise variance
colnames(var.fit)=c("fit_1", "fit_2", "fit_3", "fit_4", "fit_5")
all_y=list(ols.lm.fit, ols.cu.fit, bs.fit, ncs.fit, ss.fit)
for(i in 1:length(all_y)){
data=all_y[[i]]
var.fit[,i]=apply(data, 1, var)
}
var.fit[,6]=x; colnames(var.fit)[6]="X" # Add X into the df
var.fit[1:5,]
var.fit2=reshape(var.fit, v.names = "Value", direction = "long",
varying = colnames(var.fit)[1:5], timevar = "Method") # Reshape var.fit to long form
var.fit2[,4]=NULL
var.fit2$Method=factor(var.fit2$Method)
levels(var.fit2$Method)=c("Glocal Linear","Global Cubic Polynomial", "Cubic Spline-2 Knots", "Natural Cubic Splines-5 Knotes","Smooth Splines with GCV")
ggplot(data=var.fit2, aes(x=X, y=Value))+geom_line(aes(col=Method,group=Method))+
geom_point(aes(col=Method,group=Method))+
scale_color_manual("Methods", values = c("orange", "red", "green","blue","purple"))+ylab("Pointwise Variances") + theme_bw()
data=SAheart
dim(data)
summary(data)
# Split training and test sets
train=data[1:300,]
test=data[301:nrow(data),]
## Logistic regression
log.reg=glm(chd~., data=train, family = binomial())
log.probs=predict(log.reg, newdata=test[,1:9]) # Predicted probs of test data
log.pred=rep(1, 162)
log.pred[log.probs<0.5]=0 # Define Prob < 0.5 -- chd = 0, Prob ≥ 0.5 -- chd = 1
table(log.pred, test$chd)
log.te=mean(log.pred!=test$chd); log.te # Test error rate = 0.2469
log.se=sd(log.pred==test$chd)/sqrt(162); log.se # S.E. = 0.034
## LDA
lda.fit=lda(chd~., data=train)
lda.fit
lda.pred=predict(lda.fit, newdata = test[,1:9])
table(lda.pred$class, test$chd)
lda.te=mean(lda.pred$class!=test$chd); lda.te # Test error rate = 0.2531
lda.se=sd(lda.pred$class!=test$chd)/sqrt(162); lda.se # S.E. = 0.034
## QDA
qda.fit=qda(chd~., data=train)
qda.fit
qda.pred=predict(qda.fit, newdata = test[,1:9])
table(qda.pred$class, test$chd)
qda.te=mean(qda.pred$class!=test$chd); qda.te # Test error rate =  0.2592
qda.se=sd(qda.pred$class!=test$chd)/sqrt(162); qda.se # S.E. = 0.035
sum.table=data.frame(Logistic=c(log.te, log.se),
LDA=c(lda.te, lda.se),
QDA=c(qda.te, qda.se))
row.names(sum.table)=c("Training error", "Training error s.e.")
sum.table
library(EBImage)
library(fastICA)
rm(list=ls())
### Import 4 images
test1=readImage("./Test1_g.jpg")
setwd("~/Documents/ica_demo/")
### Import 4 images
test1=readImage("./Test1_g.jpg")
test2=readImage("./Test2_g.jpg")
test3=readImage("./Test3_g.jpg")
test4=readImage("./Test4_g.jpg")
display(test1); display(test2); display(test3); display(test4) # Take a look at them
### A linear transformation of test1 and test2
img1=0.4*test1+0.6*test2
img2=0.7*test1+0.2*test2
display(img1); display(img2)
## Convert the two transformations to vectors
img1_vec=as.vector(img1)
img2_vec=as.vector(img2)
## Construct the mixture data frame
S1 <- cbind(img1_vec, img2_vec)
# FastICA
set.seed(2017)
ICA1=fastICA(S1, 2,  alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "C", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
img1_ica=as.Image(matrix(ICA1$S[,1], nrow=600)) # Reconstruct image
img2_ica=as.Image(matrix(ICA1$S[,2], nrow=600))
ica1_img=combine(img1, img1_ica, test2,
img2,img2_ica, test1) # Combine images
plot(ica1_img, all=TRUE) # Plot all
names=c("Mixture", "ICA", "Source")
for(i in 1:3){ # Label each panel
name=names[i]
for(j in 1:2){
text(x=600*(i-1)+20, y=600*(j-1)+20, label=paste(name, " ", j, sep = ""),
cex=1, adj = c(0,1), col = "red")
}
}
# FastICA
set.seed(1)
ICA1=fastICA(S1, 2,  alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "C", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
img1_ica=as.Image(matrix(ICA1$S[,1], nrow=600)) # Reconstruct image
img2_ica=as.Image(matrix(ICA1$S[,2], nrow=600))
ica1_img=combine(img1, img1_ica, test2,
img2,img2_ica, test1) # Combine images
plot(ica1_img, all=TRUE) # Plot all
test=max(img1_ica)-img1_ica
plot(test)
plot(test, img1_ica)
plot(ica1_img, all=TRUE) # Plot all
test=1-img1_ica
plot(test, img1_ica)
plot(test)
plot(ica1_img, all=TRUE) # Plot all
for(i in 1:3){ # Label each panel
name=names[i]
for(j in 1:2){
text(x=600*(i-1)+20, y=600*(j-1)+20, label=paste(name, " ", j, sep = ""),
cex=1, adj = c(0,1), col = "red")
}
}
img1_ica.iv=1-img1_ica # Inverse grayscale values
plot(combine(img1_ica, img1_ica.iv, test2), all=T)
text(x=20, y=20, label="ICA 1")
text(x=620, y=20, label="ICA 1 Inverse")
text(x=1220, y=20, label="Source 1")
plot(combine(img1_ica, img1_ica.iv, test2), all=T)
text(x=20, y=20, label="ICA 1", cex=1, adj = c(0,1), col = "red")
text(x=620, y=20, label="ICA 1 Inverse", cex=1, adj = c(0,1), col = "red")
text(x=20, y=620, label="Source 1", cex=1, adj = c(0,1), col = "red")
### A nonlinear transformation test.
w = makeBrush(size = 21, shape = 'gaussian', sigma = 5) # Create brush
t2.f=filter2(test2, w); display(t2.f) # Blur image 2 to create non-linear transformation
img2.nl=0.2*test1+0.8*t2.f
display(img1); display(img2.nl)
img1_vec=as.vector(img1) # Convert to vector, default is by row
img2_vec.nl=as.vector(img2.nl)
S2 <- cbind(img1_vec, img2_vec.nl)
set.seed(1)
ICA2=fastICA(S2, 2,  alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "C", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
img1_ica2=as.Image(matrix(ICA2$S[,1], nrow=600))
img2_ica2=as.Image(matrix(ICA2$S[,2], nrow=600))
ica1_img=combine(img1, img1_ica2, test2,
img2.nl,img2_ica2, test1) # Combine images
plot(ica1_img, all=TRUE) # Plot all
#names=c("Mixture", "ICA", "Source")
for(i in 1:3){ # Label each panel
name=names[i]
for(j in 1:2){
text(x=600*(i-1)+20, y=600*(j-1)+20, label=paste(name, " ", j, sep = ""),
cex=1, adj = c(0,1), col = "red")
}
}
set.seed(10)
ICA2=fastICA(S2, 2,  alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "C", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
img1_ica2=as.Image(matrix(ICA2$S[,1], nrow=600))
img2_ica2=as.Image(matrix(ICA2$S[,2], nrow=600))
ica1_img=combine(img1, img1_ica2, test2,
img2.nl,img2_ica2, test1) # Combine images
plot(ica1_img, all=TRUE) # Plot all
set.seed(2017)
ICA2=fastICA(S2, 2,  alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "C", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
img1_ica2=as.Image(matrix(ICA2$S[,1], nrow=600))
img2_ica2=as.Image(matrix(ICA2$S[,2], nrow=600))
ica1_img=combine(img1, img1_ica2, test2,
img2.nl,img2_ica2, test1) # Combine images
plot(ica1_img, all=TRUE) # Plot all
#names=c("Mixture", "ICA", "Source")
for(i in 1:3){ # Label each panel
name=names[i]
for(j in 1:2){
text(x=600*(i-1)+20, y=600*(j-1)+20, label=paste(name, " ", j, sep = ""),
cex=1, adj = c(0,1), col = "red")
}
}
### A linear transformation of test1, 3 and 4
img1=0.6*test1+0.2*test3+0.2*test4
img2=0.2*test1+0.7*test3+0.1*test4
img3=0.1*test1+0.2*test3+0.7*test4
display(img1); display(img2); display(img3)
## Convert the two transformations to vectors
img1_vec=as.vector(img1)
img2_vec=as.vector(img2)
img3_vec=as.vector(img3)
## Construct the mixture data frame
S3 <- cbind(img1_vec, img2_vec, img3_vec)
# FastICA
set.seed(1)
ICA3=fastICA(S3, 3,  alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "C", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
img1_ica3=as.Image(matrix(ICA3$S[,1], nrow=600)) # Reconstruct image
img2_ica3=as.Image(matrix(ICA3$S[,2], nrow=600))
img3_ica3=as.Image(matrix(ICA3$S[,3], nrow=600))
ica3_img=combine(img1, img2, img3,
img1_ica3, img2_ica3, img3_ica3,
test1, test3, test4)
plot(ica3_img, all=TRUE)
# FastICA
set.seed(10)
ICA3=fastICA(S3, 3,  alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "C", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
img1_ica3=as.Image(matrix(ICA3$S[,1], nrow=600)) # Reconstruct image
img2_ica3=as.Image(matrix(ICA3$S[,2], nrow=600))
img3_ica3=as.Image(matrix(ICA3$S[,3], nrow=600))
ica3_img=combine(img1, img2, img3,
img1_ica3, img2_ica3, img3_ica3,
test1, test3, test4)
plot(ica3_img, all=TRUE)
# FastICA
set.seed(2017)
ICA3=fastICA(S3, 3,  alg.typ = "parallel", fun = "logcosh", alpha = 1,
method = "C", row.norm = FALSE, maxit = 200,
tol = 0.0001, verbose = TRUE)
img1_ica3=as.Image(matrix(ICA3$S[,1], nrow=600)) # Reconstruct image
img2_ica3=as.Image(matrix(ICA3$S[,2], nrow=600))
img3_ica3=as.Image(matrix(ICA3$S[,3], nrow=600))
ica3_img=combine(img1, img2, img3,
img1_ica3, img2_ica3, img3_ica3,
test1, test3, test4)
plot(ica3_img, all=TRUE)
ica3_img=combine(img1, img2, img3,
img1_ica3, img2_ica3, img3_ica3,
test2, test1, test4)
plot(ica3_img, all=TRUE)
ica3_img=combine(img1, img2, img3,
img1_ica3, img2_ica3, img3_ica3,
test3, test1, test4)
plot(ica3_img, all=TRUE)
#names=c("Mixture", "ICA", "Source")
for(i in 1:3){
name=names[i]
for(j in 1:3){
text(x=600*(j-1)+20, y=600*(i-1)+20, label=paste(name, " ", j, sep = ""),
cex=1, adj = c(0,1), col = "red")
}
}
